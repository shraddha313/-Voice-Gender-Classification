{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50         2\n",
      "           1       0.67      0.67      0.67         3\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.58      0.58      0.58         5\n",
      "weighted avg       0.60      0.60      0.60         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "import numpy as np \n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch # import PyTorch\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_mfcc(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    data, samplerate = sf.read(\"temp.wav\")\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y=data, sr=samplerate, n_mfcc=40)\n",
    "    mfccs_processed = np.mean(mfccs.T,axis=0)\n",
    "    \n",
    "    return mfccs_processed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "directory = 'F:\\speech_sri'\n",
    "output_directory = 'F:\\sppech_sri_save_feature'  # replace with your output directory\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "for file in wav_files:\n",
    "    mfccs = extract_mfcc(file)\n",
    "    \n",
    "    # Convert array to torch tensor\n",
    "    mfccs = torch.tensor(mfccs)\n",
    "\n",
    "    # Construct output file name by replacing .wav with .pt and changing the directory\n",
    "    output_file = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '.pt'))\n",
    "    \n",
    "    # Save to a PyTorch file (.pt)\n",
    "    torch.save(mfccs, output_file)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize data and labels list\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features\n",
    "for i in range(22):\n",
    "    feature = torch.load(os.path.join(output_directory, f'{i}.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i < 11 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "import numpy as np \n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch # import PyTorch\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = 'F:\\speech_sri'\n",
    "output_directory = 'F:\\sppech_sri_save_feature'  \n",
    "output_directory1= 'F:\\lfcc'\n",
    "output_directory2 = 'F:\\gfcc'\n",
    "output_directory3 = 'F:\\lpcc'\n",
    "output_directory4 = 'F:\\plpcc'\n",
    "output_directory5 = 'F:\\rp'\n",
    "output_directory6 =  'F:\\gdcc'\n",
    "output_directory7 = 'F:\\mgdcc'\n",
    "output_directory8 =  'F:\\imfcc'\n",
    "output_directory9 = 'F:\\bnf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python_speech_features in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python_speech_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'lfcc' from 'python_speech_features' (c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\python_speech_features\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpython_speech_features\u001b[39;00m \u001b[39mimport\u001b[39;00m lfcc\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwavfile\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mwav\u001b[39;00m\n\u001b[0;32m      4\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_directory1, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'lfcc' from 'python_speech_features' (c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\python_speech_features\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from python_speech_features import lfcc\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "os.makedirs(output_directory1, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_lfcc(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    (samplerate, data) = wav.read(\"temp.wav\")\n",
    "    \n",
    "    lfcc_feat = lfcc(data, samplerate, numcep=13)\n",
    "    lfccs_processed = np.mean(lfcc_feat, axis=0)\n",
    "    \n",
    "    return lfccs_processed\n",
    "\n",
    "for file in wav_files:\n",
    "    lfccs = extract_lfcc(file)\n",
    "    \n",
    "    # Convert array to torch tensor\n",
    "    lfccs = torch.tensor(lfccs)\n",
    "\n",
    "    # Construct output file name by replacing .wav with .pt and changing the directory\n",
    "    output_file = os.path.join(output_directory1, os.path.basename(file).replace('.m4a', '.pt'))\n",
    "    \n",
    "    # Save to a PyTorch file (.pt)\n",
    "    torch.save(lfccs, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting essentia\n",
      "  Downloading essentia-2.1b6.dev234.tar.gz (1.1 MB)\n",
      "                                              0.0/1.1 MB ? eta -:--:--\n",
      "     -----------                              0.3/1.1 MB 6.3 MB/s eta 0:00:01\n",
      "     --------------------------               0.7/1.1 MB 9.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 8.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/bd/10/57da0f50d2a5e1635a463cc6495089f17ee59508e56d8f7ab4e198f7c08c/essentia-2.1b6.dev234.tar.gz (from https://pypi.org/simple/essentia/): Requested essentia from https://files.pythonhosted.org/packages/bd/10/57da0f50d2a5e1635a463cc6495089f17ee59508e56d8f7ab4e198f7c08c/essentia-2.1b6.dev234.tar.gz has inconsistent version: expected '2.1b6.dev234', but metadata has '2.1b6.dev0'\n",
      "  Downloading essentia-2.1b6.dev184.tar.gz (1.1 MB)\n",
      "                                              0.0/1.1 MB ? eta -:--:--\n",
      "     ---------------                          0.4/1.1 MB 8.9 MB/s eta 0:00:01\n",
      "     -----------------------------------      1.0/1.1 MB 10.2 MB/s eta 0:00:01\n",
      "     --------------------------------------   1.0/1.1 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 4.9 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/c2/83/3d3ce935032c72db531c7456fffe090dfce1d702aa8123873fdf049f0621/essentia-2.1b6.dev184.tar.gz (from https://pypi.org/simple/essentia/): Requested essentia from https://files.pythonhosted.org/packages/c2/83/3d3ce935032c72db531c7456fffe090dfce1d702aa8123873fdf049f0621/essentia-2.1b6.dev184.tar.gz has inconsistent version: expected '2.1b6.dev184', but metadata has '2.1b6.dev0'\n",
      "  Downloading essentia-2.1b6.dev90.tar.gz (1.1 MB)\n",
      "                                              0.0/1.1 MB ? eta -:--:--\n",
      "     ---------------                          0.4/1.1 MB 12.9 MB/s eta 0:00:01\n",
      "     ---------------------------------        0.9/1.1 MB 9.5 MB/s eta 0:00:01\n",
      "     --------------------------------------   1.0/1.1 MB 9.5 MB/s eta 0:00:01\n",
      "     --------------------------------------   1.0/1.1 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 4.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Discarding https://files.pythonhosted.org/packages/84/b8/63aa9413681bf701dd6569a378c63db530a96b99ed5df367255a3a11dc42/essentia-2.1b6.dev90.tar.gz (from https://pypi.org/simple/essentia/): Requested essentia from https://files.pythonhosted.org/packages/84/b8/63aa9413681bf701dd6569a378c63db530a96b99ed5df367255a3a11dc42/essentia-2.1b6.dev90.tar.gz has inconsistent version: expected '2.1b6.dev90', but metadata has '2.1b6.dev0'\n",
      "  Downloading essentia-2.1b5.tar.gz (933 kB)\n",
      "                                              0.0/933.4 kB ? eta -:--:--\n",
      "     -----------                            286.7/933.4 kB 8.6 MB/s eta 0:00:01\n",
      "     ----------------------                 563.2/933.4 kB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  931.8/933.4 kB 8.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 933.4/933.4 kB 7.4 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from essentia) (1.23.5)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from essentia) (1.16.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from essentia) (6.0)\n",
      "Building wheels for collected packages: essentia\n",
      "  Building wheel for essentia (pyproject.toml): started\n",
      "  Building wheel for essentia (pyproject.toml): finished with status 'error'\n",
      "Failed to build essentia\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for essentia (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [73 lines of output]\n",
      "      C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\dist.py:509: InformationOnly: Normalizing '2.1-beta5' to '2.1b5'\n",
      "        self.metadata.version = self._normalize_version(\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      'rm' is not recognized as an internal or external command,\n",
      "      operable program or batch file.\n",
      "      '.' is not recognized as an internal or external command,\n",
      "      operable program or batch file.\n",
      "      C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-install-acc1q_98\\essentia_5a641da74cde4c979b81e2fcef9d8fd4\\wscript:301: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "        if ctx.env.CXX_NAME is not \"clang\":\n",
      "      \u001b[32m\u001b[0mSetting top to                           :\u001b[0m \u001b[0m\u001b[32m\u001b[32mC:\\Users\\Dell\\AppData\\Local\\Temp\\pip-install-acc1q_98\\essentia_5a641da74cde4c979b81e2fcef9d8fd4\u001b[0m \u001b[0m\n",
      "      \u001b[32m\u001b[0mSetting out to                           :\u001b[0m \u001b[0m\u001b[32m\u001b[32mC:\\Users\\Dell\\AppData\\Local\\Temp\\pip-install-acc1q_98\\essentia_5a641da74cde4c979b81e2fcef9d8fd4\\build\u001b[0m \u001b[0m\n",
      "      â†’ configuring the project in C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-install-acc1q_98\\essentia_5a641da74cde4c979b81e2fcef9d8fd4\n",
      "      â†’ Building in release mode\n",
      "      Building on win32\n",
      "      \u001b[32m\u001b[0mChecking for 'msvc' (C++ compiler)       :\u001b[0m \u001b[0m\u001b[32m\u001b[33mnot found\u001b[0m \u001b[0m\n",
      "      \u001b[32m\u001b[0mChecking for 'g++' (C++ compiler)        :\u001b[0m \u001b[0m\u001b[32m\u001b[33mnot found\u001b[0m \u001b[0m\n",
      "      \u001b[32m\u001b[0mChecking for 'clang++' (C++ compiler)    :\u001b[0m \u001b[0m\u001b[32m\u001b[33mnot found\u001b[0m \u001b[0m\n",
      "      \u001b[01;31mcould not configure a C++ compiler!\n",
      "      (complete log in C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-install-acc1q_98\\essentia_5a641da74cde4c979b81e2fcef9d8fd4\\build\\config.log)\u001b[0m\n",
      "      C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-install-acc1q_98\\essentia_5a641da74cde4c979b81e2fcef9d8fd4\\wscript:301: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "        if ctx.env.CXX_NAME is not \"clang\":\n",
      "      \u001b[01;31mThe project was not configured: run \"waf configure\" first!\u001b[0m\n",
      "      C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-install-acc1q_98\\essentia_5a641da74cde4c979b81e2fcef9d8fd4\\wscript:301: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "        if ctx.env.CXX_NAME is not \"clang\":\n",
      "      \u001b[01;31mThe project was not configured: run \"waf configure\" first!\u001b[0m\n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "        File \"c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 251, in build_wheel\n",
      "          return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 416, in build_wheel\n",
      "          return self._build_with_temp_dir(['bdist_wheel'], '.whl',\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 401, in _build_with_temp_dir\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 487, in run_setup\n",
      "          super(_BuildMetaLegacyBackend,\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 338, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 95, in <module>\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 107, in setup\n",
      "          return distutils.core.setup(**attrs)\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 185, in setup\n",
      "          return run_commands(dist)\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\core.py\", line 201, in run_commands\n",
      "          dist.run_commands()\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 969, in run_commands\n",
      "          self.run_command(cmd)\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 1234, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\wheel\\bdist_wheel.py\", line 343, in run\n",
      "          self.run_command(\"build\")\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 318, in run_command\n",
      "          self.distribution.run_command(command)\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 1234, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\command\\build.py\", line 131, in run\n",
      "          self.run_command(cmd_name)\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\cmd.py\", line 318, in run_command\n",
      "          self.distribution.run_command(command)\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\dist.py\", line 1234, in run_command\n",
      "          super().run_command(command)\n",
      "        File \"C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-env-y9eu5tm3\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py\", line 988, in run_command\n",
      "          cmd_obj.run()\n",
      "        File \"<string>\", line 46, in run\n",
      "      IndexError: list index out of range\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for essentia\n",
      "ERROR: Could not build wheels for essentia, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install essentia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'essentia'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39messentia\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstandard\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mes\u001b[39;00m\n\u001b[0;32m      4\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_directory1, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m wav_files \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root, file)\n\u001b[0;32m      7\u001b[0m              \u001b[39mfor\u001b[39;00m root, dirs, files \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mwalk(directory)\n\u001b[0;32m      8\u001b[0m              \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m fnmatch\u001b[39m.\u001b[39mfilter(files, \u001b[39m'\u001b[39m\u001b[39m*.m4a\u001b[39m\u001b[39m'\u001b[39m)]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'essentia'"
     ]
    }
   ],
   "source": [
    "import essentia.standard as es\n",
    "\n",
    "\n",
    "os.makedirs(output_directory1, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_gfcc(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    loader = es.MonoLoader(filename='temp.wav')\n",
    "    audio = loader()\n",
    "    \n",
    "    gfcc = es.GFCC(numberCoefficients=13)\n",
    "    gfcc_bands, gfcc_coeffs = gfcc(es.MelBands()(audio))\n",
    "    gfccs_processed = np.mean(gfcc_coeffs, axis=0)\n",
    "    \n",
    "    return gfccs_processed\n",
    "\n",
    "for file in wav_files:\n",
    "    gfccs = extract_gfcc(file)\n",
    "    \n",
    "    # Convert array to torch tensor\n",
    "    gfccs = torch.tensor(gfccs)\n",
    "\n",
    "    # Construct output file name by replacing .wav with .pt and changing the directory\n",
    "    output_file = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '.pt'))\n",
    "    \n",
    "    # Save to a PyTorch file (.pt)\n",
    "    torch.save(gfccs, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spafe\n",
      "  Downloading spafe-0.3.2-py3-none-any.whl (93 kB)\n",
      "                                              0.0/93.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 93.5/93.5 kB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spafe) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.7.3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spafe) (1.10.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from spafe) (4.5.0)\n",
      "Installing collected packages: spafe\n",
      "Successfully installed spafe-0.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U spafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from spafe.features.gfcc import gfcc\n",
    "from spafe.utils import vis\n",
    "import scipy.io.wavfile as wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(output_directory2, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_gfcc(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    (samplerate, data) = wav.read(\"temp.wav\")\n",
    "    \n",
    "    \n",
    "    gfccs = gfcc(sig=data, fs=samplerate, num_ceps=13)\n",
    "   \n",
    "    gfccs_processed = np.mean(gfccs, axis=0)\n",
    "    \n",
    "    return  gfccs_processed\n",
    "\n",
    "for file in wav_files:\n",
    "    gfccs = extract_gfcc(file)\n",
    "    \n",
    "    # Convert array to torch tensor\n",
    "    \n",
    "    gfccs = torch.tensor(gfccs)\n",
    "\n",
    "    # Construct output file name by replacing .wav with .pt and changing the directory\n",
    "    \n",
    "    output_file_gfcc = os.path.join(output_directory2, os.path.basename(file).replace('.m4a', '_gfcc.pt'))\n",
    "    \n",
    "    # Save to a PyTorch file (.pt)\n",
    "   \n",
    "    torch.save(gfccs, output_file_gfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features\n",
    "for i in range(22):\n",
    "    feature = torch.load(os.path.join(output_directory2, f'{i}_gfcc.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i < 11 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.88      0.75      0.76         5\n",
      "weighted avg       0.85      0.80      0.78         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize data and labels list\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features\n",
    "for i in range(22):\n",
    "    feature = torch.load(os.path.join(output_directory2, f'{i}_gfcc.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i < 11 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = svm.SVC(kernel='linear')  # linear kernel\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67         4\n",
      "           1       0.75      0.60      0.67         5\n",
      "\n",
      "    accuracy                           0.67         9\n",
      "   macro avg       0.68      0.68      0.67         9\n",
      "weighted avg       0.68      0.67      0.67         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize data and labels list\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features\n",
    "for i in range(22):\n",
    "    feature = torch.load(os.path.join(output_directory2, f'{i}_gfcc.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i < 11 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.4, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = svm.SVC(kernel='linear')  # linear kernel\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spafe.features.bfcc import bfcc\n",
    "from spafe.features.lpc import lpc\n",
    "from spafe.features.pncc import pncc\n",
    "from spafe.features.rplp import rplp\n",
    "from spafe.features.ngcc import ngcc\n",
    "from spafe.features.msrcc import msrcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "os.makedirs(output_directory3, exist_ok=True)\n",
    "os.makedirs(output_directory4, exist_ok=True)\n",
    "\n",
    "os.makedirs(output_directory6, exist_ok=True)\n",
    "os.makedirs(output_directory7, exist_ok=True)\n",
    "os.makedirs(output_directory8, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    (samplerate, data) = wav.read(\"temp.wav\")\n",
    "    \n",
    "    bfccs = bfcc(sig=data, fs=samplerate, num_ceps=13)\n",
    "    \n",
    "    pnccs = pncc(sig=data, fs=samplerate, num_ceps=13)\n",
    "   \n",
    "    ngccs = ngcc(sig=data, fs=samplerate, num_ceps=13)\n",
    "    msrccs = msrcc(sig=data, fs=samplerate, num_ceps=13)\n",
    "    \n",
    "    # Compute the mean along the time axis (axis 0) to get a single feature vector for each file\n",
    "    bfccs_processed = np.mean(bfccs, axis=0)\n",
    "  \n",
    "    pnccs_processed = np.mean(pnccs, axis=0)\n",
    "    \n",
    "    ngccs_processed = np.mean(ngccs, axis=0)\n",
    "    msrccs_processed = np.mean(msrccs, axis=0)\n",
    "    \n",
    "    return bfccs_processed, pnccs_processed, ngccs_processed, msrccs_processed\n",
    "\n",
    "for file in wav_files:\n",
    "    bfccs,  pnccs, ngccs, msrccs = extract_features(file)\n",
    "    \n",
    "    # Convert arrays to torch tensors\n",
    "    bfccs = torch.tensor(bfccs)\n",
    "    \n",
    "    pnccs = torch.tensor(pnccs)\n",
    "    \n",
    "    ngccs = torch.tensor(ngccs)\n",
    "    msrccs = torch.tensor(msrccs)\n",
    "\n",
    "    # Construct output file names by replacing .wav with .pt and changing the directory\n",
    "    output_file_bfcc = os.path.join(output_directory3, os.path.basename(file).replace('.m4a', '_bfcc.pt'))\n",
    "    \n",
    "    output_file_pncc = os.path.join(output_directory4, os.path.basename(file).replace('.m4a', '_pncc.pt'))\n",
    "   \n",
    "    output_file_ngcc = os.path.join(output_directory7, os.path.basename(file).replace('.m4a', '_ngcc.pt'))\n",
    "    output_file_msrcc = os.path.join(output_directory8, os.path.basename(file).replace('.m4a', '_msrcc.pt'))\n",
    "    \n",
    "    torch.save(bfccs, output_file_bfcc)\n",
    "    \n",
    "    torch.save(pnccs, output_file_pncc)\n",
    "    \n",
    "    torch.save(ngccs, output_file_ngcc)\n",
    "    torch.save(msrccs, output_file_msrcc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80         2\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.83      0.83      0.80         5\n",
      "weighted avg       0.87      0.80      0.80         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize data and labels list\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features\n",
    "for i in range(22):\n",
    "    feature = torch.load(os.path.join(output_directory3, f'{i}_bfcc.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i < 11 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = svm.SVC(kernel='linear')  # linear kernel\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.88      0.75      0.76         5\n",
      "weighted avg       0.85      0.80      0.78         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize data and labels list\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features\n",
    "for i in range(22):\n",
    "    feature = torch.load(os.path.join(output_directory7, f'{i}_ngcc.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i < 11 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = svm.SVC(kernel='linear')  # linear kernel\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.33      0.50         3\n",
      "           1       0.67      1.00      0.80         4\n",
      "\n",
      "    accuracy                           0.71         7\n",
      "   macro avg       0.83      0.67      0.65         7\n",
      "weighted avg       0.81      0.71      0.67         7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize data and labels list\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features[]\n",
    "for i in range(22):\n",
    "    feature = torch.load(os.path.join(output_directory7, f'{i}_ngcc.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i < 11 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = svm.SVC(kernel='linear')  # linear kernel\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
