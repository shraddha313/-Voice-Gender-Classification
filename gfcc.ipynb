{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "import numpy as np \n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch # import PyTorch\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spafe.features.gfcc import gfcc\n",
    "from spafe.utils import vis\n",
    "import scipy.io.wavfile as wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m  gfccs_processed\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m wav_files:\n\u001b[1;32m---> 23\u001b[0m     gfccs \u001b[39m=\u001b[39m extract_gfcc(file)\n\u001b[0;32m     25\u001b[0m     \u001b[39m# Convert array to torch tensor\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     gfccs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(gfccs)\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mextract_gfcc\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_gfcc\u001b[39m(file_name):\n\u001b[0;32m     10\u001b[0m     \u001b[39m# Convert m4a file to wav            \u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     audio \u001b[39m=\u001b[39m AudioSegment\u001b[39m.\u001b[39;49mfrom_file(file_name)\n\u001b[0;32m     12\u001b[0m     audio\u001b[39m.\u001b[39mexport(\u001b[39m\"\u001b[39m\u001b[39mtemp.wav\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwav\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m     (samplerate, data) \u001b[39m=\u001b[39m wav\u001b[39m.\u001b[39mread(\u001b[39m\"\u001b[39m\u001b[39mtemp.wav\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pydub\\audio_segment.py:768\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[1;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m log_conversion(conversion_command)\n\u001b[0;32m    766\u001b[0m p \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mPopen(conversion_command, stdin\u001b[39m=\u001b[39mstdin_parameter,\n\u001b[0;32m    767\u001b[0m                      stdout\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mPIPE, stderr\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mPIPE)\n\u001b[1;32m--> 768\u001b[0m p_out, p_err \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mcommunicate(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mstdin_data)\n\u001b[0;32m    770\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(p_out) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    771\u001b[0m     \u001b[39mif\u001b[39;00m close_file:\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py:1134\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     endtime \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m     stdout, stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate(\u001b[39minput\u001b[39;49m, endtime, timeout)\n\u001b[0;32m   1135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m     \u001b[39m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py:1508\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1504\u001b[0m \u001b[39m# Wait for the reader threads, or time out.  If we time out, the\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[39m# threads remain reading and the fds left open in case the user\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m \u001b[39m# calls communicate again.\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1508\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstdout_thread\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_remaining_time(endtime))\n\u001b[0;32m   1509\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m   1510\u001b[0m         \u001b[39mraise\u001b[39;00m TimeoutExpired(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, orig_timeout)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py:1053\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1053\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[0;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1055\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py:1073\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1073\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[0;32m   1074\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m   1075\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "directory = r'F:\\dataset_sri'\n",
    "output_directory= r'F:\\data_gfcc'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_gfcc(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    (samplerate, data) = wav.read(\"temp.wav\")\n",
    "    \n",
    "    \n",
    "    gfccs = gfcc(sig=data, fs=samplerate, num_ceps=13)\n",
    "   \n",
    "    gfccs_processed = np.mean(gfccs, axis=0)\n",
    "    \n",
    "    return  gfccs_processed\n",
    "\n",
    "for file in wav_files:\n",
    "    gfccs = extract_gfcc(file)\n",
    "    \n",
    "    # Convert array to torch tensor\n",
    "    \n",
    "    gfccs = torch.tensor(gfccs)\n",
    "\n",
    "    # Construct output file name by replacing .wav with .pt and changing the directory\n",
    "    \n",
    "    output_file_gfcc = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '_gfcc.pt'))\n",
    "    \n",
    "    # Save to a PyTorch file (.pt)\n",
    "   \n",
    "    torch.save(gfccs, output_file_gfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5993\n",
      "5993\n",
      "5993\n",
      "5993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85      1069\n",
      "           1       0.81      0.71      0.75       729\n",
      "\n",
      "    accuracy                           0.81      1798\n",
      "   macro avg       0.81      0.80      0.80      1798\n",
      "weighted avg       0.81      0.81      0.81      1798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "output_directory= r'F:\\data_gfcc'\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features\n",
    "for i in range(5993):\n",
    "    feature = torch.load(os.path.join(output_directory, f'{i}_gfcc.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i <= 2310 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5993\n",
      "5993\n",
      "5993\n",
      "5993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84       711\n",
      "           1       0.79      0.70      0.74       488\n",
      "\n",
      "    accuracy                           0.80      1199\n",
      "   macro avg       0.80      0.79      0.79      1199\n",
      "weighted avg       0.80      0.80      0.80      1199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "output_directory= r'F:\\data_gfcc'\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features\n",
    "for i in range(5993):\n",
    "    feature = torch.load(os.path.join(output_directory, f'{i}_gfcc.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i <= 2310 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5993\n",
      "5993\n",
      "5993\n",
      "5993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85      1445\n",
      "           1       0.80      0.72      0.76       953\n",
      "\n",
      "    accuracy                           0.82      2398\n",
      "   macro avg       0.81      0.80      0.81      2398\n",
      "weighted avg       0.82      0.82      0.82      2398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "output_directory= r'F:\\data_gfcc'\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features\n",
    "for i in range(5993):\n",
    "    feature = torch.load(os.path.join(output_directory, f'{i}_gfcc.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i <= 2310 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.4, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "# model = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (23.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (67.6.1)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-68.0.0-py3-none-any.whl (804 kB)\n",
      "Requirement already satisfied: wheel in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.40.0)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 67.6.1\n",
      "    Uninstalling setuptools-67.6.1:\n",
      "      Successfully uninstalled setuptools-67.6.1\n",
      "Successfully installed setuptools-68.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting parselmouth\n",
      "  Using cached parselmouth-1.1.1.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting googleads==3.8.0 (from parselmouth)\n",
      "  Using cached googleads-3.8.0.tar.gz (23 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [1 lines of output]\n",
      "      error in googleads setup command: use_2to3 is invalid.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "! pip install parselmouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'parselmouth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlibrosa\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydub\u001b[39;00m \u001b[39mimport\u001b[39;00m AudioSegment\n\u001b[1;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mparselmouth\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mparselmouth\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpraat\u001b[39;00m \u001b[39mimport\u001b[39;00m call\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyAudioAnalysis\u001b[39;00m \u001b[39mimport\u001b[39;00m audioBasicIO\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'parselmouth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import audioFeatureExtraction\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "# Directory containing audio files\n",
    "directory = r'F:\\dataset_sri'\n",
    "output_directory= r'F:\\data_pitch'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Get list of all .m4a files in the directory\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_features(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    (samplerate, data) = wav.read(\"temp.wav\")\n",
    "\n",
    "    # Extract pitch using librosa\n",
    "    pitch = librosa.yin(data, librosa.note_to_hz('C2'), librosa.note_to_hz('C7'))\n",
    "    avg_pitch = pitch.mean()\n",
    "\n",
    "    # Extract formant frequencies using parselmouth\n",
    "    snd = parselmouth.Sound(\"temp.wav\")\n",
    "    formants = call(snd, \"To Formant (burg)\", 0.0, 5, 5500, 0.025, 50)\n",
    "    num_formants = call(formants, \"Get number of formants\", 0.5)\n",
    "    formant_frequencies = [call(formants, \"Get value at time\", 1, formant_num, 0.5, 'Hertz') for formant_num in range(1, num_formants+1)]\n",
    "    avg_formant_frequency = np.mean(formant_frequencies)\n",
    "\n",
    "    # Extract jitter and shimmer using pyAudioAnalysis\n",
    "    [Fs, x] = audioBasicIO.read_audio_file(\"temp.wav\")\n",
    "    F, f_names = audioFeatureExtraction.stFeatureExtraction(x, Fs, 0.050*Fs, 0.025*Fs)\n",
    "    jitter = np.mean(F[f_names.index('jitter')])\n",
    "    shimmer = np.mean(F[f_names.index('shimmer')])\n",
    "\n",
    "    # Return extracted features\n",
    "    return  avg_pitch, avg_formant_frequency, jitter, shimmer\n",
    "\n",
    "# Process all files\n",
    "for file in wav_files:\n",
    "    avg_pitch, avg_formant_frequency, jitter, shimmer = extract_features(file)\n",
    "\n",
    "    # Convert values to torch tensors\n",
    "    avg_pitch = torch.tensor(avg_pitch)\n",
    "    avg_formant_frequency = torch.tensor(avg_formant_frequency)\n",
    "    jitter = torch.tensor(jitter)\n",
    "    shimmer = torch.tensor(shimmer)\n",
    "\n",
    "    # Club all features together\n",
    "    features = {'avg_pitch': avg_pitch, 'avg_formant_frequency': avg_formant_frequency, 'jitter': jitter, 'shimmer': shimmer}\n",
    "\n",
    "    # Construct output file name by replacing .wav with .pt and changing the directory\n",
    "    output_file = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '_features.pt'))\n",
    "\n",
    "    # Save to PyTorch files (.pt)\n",
    "    torch.save(features, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praat-parselmouth\n",
      "  Downloading praat_parselmouth-0.4.3-cp39-cp39-win_amd64.whl (8.9 MB)\n",
      "                                              0.0/8.9 MB ? eta -:--:--\n",
      "                                              0.1/8.9 MB 4.3 MB/s eta 0:00:03\n",
      "     -                                        0.3/8.9 MB 3.4 MB/s eta 0:00:03\n",
      "     -                                        0.4/8.9 MB 3.3 MB/s eta 0:00:03\n",
      "     --                                       0.6/8.9 MB 3.3 MB/s eta 0:00:03\n",
      "     ---                                      0.8/8.9 MB 3.4 MB/s eta 0:00:03\n",
      "     ---                                      0.8/8.9 MB 3.1 MB/s eta 0:00:03\n",
      "     ----                                     1.0/8.9 MB 3.0 MB/s eta 0:00:03\n",
      "     ----                                     1.0/8.9 MB 3.0 MB/s eta 0:00:03\n",
      "     -----                                    1.1/8.9 MB 2.9 MB/s eta 0:00:03\n",
      "     -----                                    1.2/8.9 MB 2.9 MB/s eta 0:00:03\n",
      "     -----                                    1.3/8.9 MB 2.7 MB/s eta 0:00:03\n",
      "     ------                                   1.4/8.9 MB 2.7 MB/s eta 0:00:03\n",
      "     ------                                   1.5/8.9 MB 2.7 MB/s eta 0:00:03\n",
      "     -------                                  1.6/8.9 MB 2.6 MB/s eta 0:00:03\n",
      "     -------                                  1.7/8.9 MB 2.6 MB/s eta 0:00:03\n",
      "     -------                                  1.8/8.9 MB 2.5 MB/s eta 0:00:03\n",
      "     --------                                 1.8/8.9 MB 2.5 MB/s eta 0:00:03\n",
      "     --------                                 1.9/8.9 MB 2.4 MB/s eta 0:00:03\n",
      "     --------                                 2.0/8.9 MB 2.3 MB/s eta 0:00:03\n",
      "     --------                                 2.0/8.9 MB 2.3 MB/s eta 0:00:03\n",
      "     ---------                                2.0/8.9 MB 2.2 MB/s eta 0:00:04\n",
      "     ---------                                2.1/8.9 MB 2.2 MB/s eta 0:00:04\n",
      "     ---------                                2.2/8.9 MB 2.2 MB/s eta 0:00:04\n",
      "     ----------                               2.3/8.9 MB 2.2 MB/s eta 0:00:04\n",
      "     ----------                               2.4/8.9 MB 2.2 MB/s eta 0:00:04\n",
      "     -----------                              2.5/8.9 MB 2.2 MB/s eta 0:00:03\n",
      "     -----------                              2.5/8.9 MB 2.1 MB/s eta 0:00:03\n",
      "     -----------                              2.7/8.9 MB 2.2 MB/s eta 0:00:03\n",
      "     ------------                             2.9/8.9 MB 2.3 MB/s eta 0:00:03\n",
      "     -------------                            3.1/8.9 MB 2.4 MB/s eta 0:00:03\n",
      "     ---------------                          3.4/8.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------                         3.6/8.9 MB 2.6 MB/s eta 0:00:03\n",
      "     -----------------                        4.0/8.9 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------------                       4.2/8.9 MB 2.8 MB/s eta 0:00:02\n",
      "     -------------------                      4.4/8.9 MB 2.8 MB/s eta 0:00:02\n",
      "     --------------------                     4.7/8.9 MB 2.9 MB/s eta 0:00:02\n",
      "     ---------------------                    4.9/8.9 MB 3.0 MB/s eta 0:00:02\n",
      "     -----------------------                  5.2/8.9 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------------                 5.4/8.9 MB 3.1 MB/s eta 0:00:02\n",
      "     -------------------------                5.7/8.9 MB 3.2 MB/s eta 0:00:02\n",
      "     --------------------------               5.9/8.9 MB 3.2 MB/s eta 0:00:01\n",
      "     ----------------------------             6.5/8.9 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------           6.7/8.9 MB 3.5 MB/s eta 0:00:01\n",
      "     ------------------------------           6.9/8.9 MB 3.5 MB/s eta 0:00:01\n",
      "     --------------------------------         7.2/8.9 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------        7.5/8.9 MB 3.6 MB/s eta 0:00:01\n",
      "     ----------------------------------       7.7/8.9 MB 3.6 MB/s eta 0:00:01\n",
      "     -----------------------------------      7.9/8.9 MB 3.6 MB/s eta 0:00:01\n",
      "     ------------------------------------     8.2/8.9 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------------    8.3/8.9 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------------    8.4/8.9 MB 3.7 MB/s eta 0:00:01\n",
      "     --------------------------------------   8.6/8.9 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.9/8.9 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.9/8.9 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.9/8.9 MB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.7.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from praat-parselmouth) (1.23.5)\n",
      "Installing collected packages: praat-parselmouth\n",
      "Successfully installed praat-parselmouth-0.4.3\n"
     ]
    }
   ],
   "source": [
    "! pip install praat-parselmouth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "PraatError",
     "evalue": "Command \"Get jitter (local)\" not available for given objects.\r",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPraatError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m# Process all files\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m wav_files:\n\u001b[1;32m---> 59\u001b[0m     avg_pitch, avg_formant_frequency, jitter, shimmer, avg_spectral_contrast \u001b[39m=\u001b[39m extract_features(file)\n\u001b[0;32m     61\u001b[0m     \u001b[39m# Convert values to torch tensors\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     avg_pitch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(avg_pitch)\n",
      "Cell \u001b[1;32mIn[16], line 37\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m     35\u001b[0m pointProcess \u001b[39m=\u001b[39m parselmouth\u001b[39m.\u001b[39mpraat\u001b[39m.\u001b[39mcall(sound, \u001b[39m\"\u001b[39m\u001b[39mTo PointProcess (periodic, cc)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m75\u001b[39m, \u001b[39m500\u001b[39m)\n\u001b[0;32m     36\u001b[0m shimmer \u001b[39m=\u001b[39m parselmouth\u001b[39m.\u001b[39mpraat\u001b[39m.\u001b[39mcall([sound, pointProcess], \u001b[39m\"\u001b[39m\u001b[39mGet shimmer (local)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m, \u001b[39m75\u001b[39m, \u001b[39m0.02\u001b[39m, \u001b[39m1.3\u001b[39m, \u001b[39m1.6\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m jitter \u001b[39m=\u001b[39m parselmouth\u001b[39m.\u001b[39;49mpraat\u001b[39m.\u001b[39;49mcall([sound, pointProcess], \u001b[39m\"\u001b[39;49m\u001b[39mGet jitter (local)\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0.0\u001b[39;49m, \u001b[39m0.0\u001b[39;49m, \u001b[39m75\u001b[39;49m, \u001b[39m0.02\u001b[39;49m, \u001b[39m1.3\u001b[39;49m, \u001b[39m1.6\u001b[39;49m)\n\u001b[0;32m     42\u001b[0m \u001b[39m# Extract pitch using librosa\u001b[39;00m\n\u001b[0;32m     43\u001b[0m y, sr \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mtemp.wav\u001b[39m\u001b[39m\"\u001b[39m, sr\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mPraatError\u001b[0m: Command \"Get jitter (local)\" not available for given objects.\r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: eyed3 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.9.7)\n",
      "Requirement already satisfied: coverage[toml]<6.0.0,>=5.3.1 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from eyed3) (5.5)\n",
      "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from eyed3) (2.1.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.0.7 in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from eyed3) (1.2.0)\n",
      "Requirement already satisfied: toml in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from coverage[toml]<6.0.0,>=5.3.1->eyed3) (0.10.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from deprecation<3.0.0,>=2.1.0->eyed3) (23.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install eyed3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyAudioAnalysis in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.3.14)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pyAudioAnalysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "yin() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39m# Process all files\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m wav_files:\n\u001b[1;32m---> 39\u001b[0m     avg_pitch, avg_spectral_contrast \u001b[39m=\u001b[39m extract_features(file)\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Convert values to torch tensors\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     avg_pitch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(avg_pitch)\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m     23\u001b[0m (samplerate, data) \u001b[39m=\u001b[39m wav\u001b[39m.\u001b[39mread(\u001b[39m\"\u001b[39m\u001b[39mtemp.wav\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[39m# Extract pitch using librosa\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m pitch \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49myin(data, librosa\u001b[39m.\u001b[39;49mnote_to_hz(\u001b[39m'\u001b[39;49m\u001b[39mC2\u001b[39;49m\u001b[39m'\u001b[39;49m), librosa\u001b[39m.\u001b[39;49mnote_to_hz(\u001b[39m'\u001b[39;49m\u001b[39mC7\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     27\u001b[0m avg_pitch \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(pitch)\n\u001b[0;32m     29\u001b[0m \u001b[39m# Simplified formant extraction with librosa\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# Using spectral contrast\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: yin() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "# Directory containing audio files\n",
    "directory = r'F:\\dataset_sri'\n",
    "output_directory = r'F:\\data_pitch'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Get list of all .m4a files in the directory\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_features(file_name):\n",
    "    # Convert m4a file to wav\n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    (samplerate, data) = wav.read(\"temp.wav\")\n",
    "\n",
    "    # Extract pitch using librosa\n",
    "    pitch = librosa.yin(data, librosa.note_to_hz('C2'), librosa.note_to_hz('C7'))\n",
    "    avg_pitch = np.mean(pitch)\n",
    "\n",
    "    # Simplified formant extraction with librosa\n",
    "    # Using spectral contrast\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=data, sr=samplerate)\n",
    "    avg_spectral_contrast = np.mean(spectral_contrast)\n",
    "\n",
    "    # Return extracted features\n",
    "    return avg_pitch, avg_spectral_contrast\n",
    "\n",
    "# Process all files\n",
    "for file in wav_files:\n",
    "    avg_pitch, avg_spectral_contrast = extract_features(file)\n",
    "\n",
    "    # Convert values to torch tensors\n",
    "    avg_pitch = torch.tensor(avg_pitch)\n",
    "    avg_spectral_contrast = torch.tensor(avg_spectral_contrast)\n",
    "\n",
    "    # Club all features together\n",
    "    features = {'avg_pitch': avg_pitch, 'avg_spectral_contrast': avg_spectral_contrast}\n",
    "\n",
    "    # Construct output file name by replacing .wav with .pt and changing the directory\n",
    "    output_file = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '_features.pt'))\n",
    "\n",
    "    # Save to PyTorch files (.pt)\n",
    "    torch.save(features, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "# Directory containing audio files\n",
    "directory = r'F:\\dataset_sri'\n",
    "output_directory = r'F:\\data_pitch'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Get list of all .m4a files in the directory\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_features(file_name):\n",
    "    # Convert m4a file to wav\n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    (samplerate, data) = wav.read(\"temp.wav\")\n",
    "\n",
    "    # Convert data to floating-point\n",
    "    data = data.astype(np.float32)\n",
    "\n",
    "    # Extract pitch using librosa\n",
    "    pitch = librosa.yin(data, fmin=75, fmax=1600)\n",
    "    avg_pitch = np.mean(pitch)\n",
    "\n",
    "    # Simplified formant extraction with librosa\n",
    "    # Using spectral contrast\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=data, sr=samplerate)\n",
    "    avg_spectral_contrast = np.mean(spectral_contrast)\n",
    "\n",
    "    # Return extracted features\n",
    "    return avg_pitch, avg_spectral_contrast\n",
    "\n",
    "# Process all files\n",
    "for file in wav_files:\n",
    "    avg_pitch, avg_spectral_contrast = extract_features(file)\n",
    "\n",
    "    # Convert values to torch tensors\n",
    "    avg_pitch = torch.tensor(avg_pitch)\n",
    "    avg_spectral_contrast = torch.tensor(avg_spectral_contrast)\n",
    "\n",
    "    # Club all features together\n",
    "    features = {'avg_pitch': avg_pitch, 'avg_spectral_contrast': avg_spectral_contrast}\n",
    "\n",
    "    # Construct output file name by replacing .wav with .pt and changing the directory\n",
    "    output_file = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '_features.pt'))\n",
    "\n",
    "    # Save to PyTorch files (.pt)\n",
    "    torch.save(features, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5993\n",
      "5993\n",
      "5993\n",
      "5993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85      1445\n",
      "           1       0.80      0.72      0.76       953\n",
      "\n",
      "    accuracy                           0.82      2398\n",
      "   macro avg       0.81      0.80      0.80      2398\n",
      "weighted avg       0.81      0.82      0.81      2398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "output_directory= r'F:\\data_gfcc'\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features\n",
    "for i in range(5993):\n",
    "    feature = torch.load(os.path.join(output_directory, f'{i}_gfcc.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i <= 2310 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.4, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "# model = LogisticRegression(solver='liblinear')\n",
    "model = svm.SVC(kernel='linear')\n",
    "# model = RandomForestClassifier(n_estimators=100)\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m directory \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mF:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdataset_sri\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      2\u001b[0m output_directory\u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mF:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata_pitch\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_directory, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m wav_files \u001b[39m=\u001b[39m [os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root, file)\n\u001b[0;32m      6\u001b[0m              \u001b[39mfor\u001b[39;00m root, dirs, files \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mwalk(directory)\n\u001b[0;32m      7\u001b[0m              \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m fnmatch\u001b[39m.\u001b[39mfilter(files, \u001b[39m'\u001b[39m\u001b[39m*.m4a\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_gfcc\u001b[39m(file_name):\n\u001b[0;32m     10\u001b[0m     \u001b[39m# Convert m4a file to wav            \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "directory = r'F:\\dataset_sri'\n",
    "output_directory= r'F:\\data_pitch'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_gfcc(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    (samplerate, data) = wav.read(\"temp.wav\")\n",
    "    \n",
    "    \n",
    "    gfccs = gfcc(sig=data, fs=samplerate, num_ceps=13)\n",
    "   \n",
    "    gfccs_processed = np.mean(gfccs, axis=0)\n",
    "    \n",
    "    return  gfccs_processed\n",
    "\n",
    "for file in wav_files:\n",
    "    gfccs = extract_gfcc(file)\n",
    "    \n",
    "    # Convert array to torch tensor\n",
    "    \n",
    "    gfccs = torch.tensor(gfccs)\n",
    "\n",
    "    # Construct output file name by replacing .wav with .pt and changing the directory\n",
    "    \n",
    "    output_file_gfcc = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '_gfcc.pt'))\n",
    "    \n",
    "    # Save to a PyTorch file (.pt)\n",
    "   \n",
    "    torch.save(gfccs, output_file_gfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5993\n",
      "5993\n",
      "5993\n",
      "5993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84      1445\n",
      "           1       0.79      0.69      0.73       953\n",
      "\n",
      "    accuracy                           0.80      2398\n",
      "   macro avg       0.80      0.78      0.79      2398\n",
      "weighted avg       0.80      0.80      0.80      2398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "output_directory= r'F:\\data_gfcc'\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load extracted features\n",
    "for i in range(5993):\n",
    "    feature = torch.load(os.path.join(output_directory, f'{i}_gfcc.pt'))\n",
    "    data.append(feature.numpy())  # Convert tensor to numpy array for sklearn\n",
    "    # Assign labels: first 11 files are female, next 11 files are male\n",
    "    labels.append(1 if i <= 2310 else 0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Split data into train and test sets\n",
    "# Ensure the number of samples are consistent between `data` and `labels`\n",
    "assert len(data) == len(labels), \"Mismatched sample sizes: {} vs {}\".format(len(data), len(labels))\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.4, random_state=42)\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "# model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print performance metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first item in data: (13, 1)\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 10s 13ms/step - loss: 0.6318 - accuracy: 0.6650 - val_loss: 0.6689 - val_accuracy: 0.6347\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.5222 - accuracy: 0.7466 - val_loss: 0.6329 - val_accuracy: 0.6522\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.4880 - accuracy: 0.7701 - val_loss: 0.5413 - val_accuracy: 0.7590\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.4785 - accuracy: 0.7751 - val_loss: 0.5196 - val_accuracy: 0.7548\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.4556 - accuracy: 0.7893 - val_loss: 0.5276 - val_accuracy: 0.7823\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.4466 - accuracy: 0.7991 - val_loss: 0.4837 - val_accuracy: 0.7731\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.4268 - accuracy: 0.8139 - val_loss: 0.4687 - val_accuracy: 0.7857\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.4254 - accuracy: 0.8125 - val_loss: 0.4965 - val_accuracy: 0.7681\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.4211 - accuracy: 0.8139 - val_loss: 0.4620 - val_accuracy: 0.7815\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.4217 - accuracy: 0.8083 - val_loss: 0.4883 - val_accuracy: 0.7990\n",
      "Test loss: 0.48827341198921204\n",
      "Test accuracy: 0.7989991903305054\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "directory = r'F:\\data_gfcc'  # Replace with the directory path containing the .pt files\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(5993):\n",
    "    filename = f'{i}_gfcc.pt'\n",
    "    if filename in os.listdir(directory):  # Ensure the file exists in the directory\n",
    "        feature = torch.load(os.path.join(directory, filename))\n",
    "        data.append(feature.view(13,1).numpy())  # Adjusted this line to match ResNet input\n",
    "\n",
    "        labels.append(1 if i <= 2310 else 0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "data  = np.array(data)\n",
    "\n",
    "print(\"Shape of the first item in data:\", data[0].shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "# Define ResNet block\n",
    "def resnet_block(inputs, num_filters, kernel_size, strides, activation='relu'):\n",
    "    x = layers.Conv1D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv1D(num_filters, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    shortcut = layers.Conv1D(num_filters, kernel_size=1, strides=strides, padding='same')(inputs)\n",
    "    shortcut = layers.BatchNormalization()(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "# Define ResNet model\n",
    "def resnet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv1D(16, kernel_size=3, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "    x = resnet_block(x, 32, 3, 2)\n",
    "    x = resnet_block(x, 32, 3, 1)\n",
    "    \n",
    "    x = resnet_block(x, 64, 3, 2)\n",
    "    x = resnet_block(x, 64, 3, 1)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = resnet(input_shape=(13, 1), num_classes=num_classes)\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first item in data: (13, 1)\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 7s 12ms/step - loss: 0.6695 - accuracy: 0.6516 - val_loss: 0.7958 - val_accuracy: 0.4095\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.5979 - accuracy: 0.6861 - val_loss: 0.8805 - val_accuracy: 0.4137\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.5764 - accuracy: 0.7094 - val_loss: 1.0980 - val_accuracy: 0.5947\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.5607 - accuracy: 0.7117 - val_loss: 0.6478 - val_accuracy: 0.6547\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.5503 - accuracy: 0.7265 - val_loss: 0.6796 - val_accuracy: 0.5988\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.5426 - accuracy: 0.7311 - val_loss: 0.6688 - val_accuracy: 0.5988\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.5356 - accuracy: 0.7274 - val_loss: 0.6331 - val_accuracy: 0.6722\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.5308 - accuracy: 0.7405 - val_loss: 0.6063 - val_accuracy: 0.6672\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.5256 - accuracy: 0.7459 - val_loss: 0.5704 - val_accuracy: 0.7114\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.5206 - accuracy: 0.7453 - val_loss: 0.7074 - val_accuracy: 0.6530\n",
      "Test loss: 0.707412600517273\n",
      "Test accuracy: 0.6530442237854004\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "directory = r'F:\\data_bfcc'  # Replace with the directory path containing the .pt files\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(5993):\n",
    "    filename = f'{i}_bfcc.pt'\n",
    "    if filename in os.listdir(directory):  # Ensure the file exists in the directory\n",
    "        feature = torch.load(os.path.join(directory, filename))\n",
    "        data.append(feature.view(13,1).numpy())  # Adjusted this line to match ResNet input\n",
    "\n",
    "        labels.append(1 if i <= 2310 else 0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "data  = np.array(data)\n",
    "\n",
    "print(\"Shape of the first item in data:\", data[0].shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "# Define ResNet block\n",
    "def resnet_block(inputs, num_filters, kernel_size, strides, activation='relu'):\n",
    "    x = layers.Conv1D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv1D(num_filters, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    shortcut = layers.Conv1D(num_filters, kernel_size=1, strides=strides, padding='same')(inputs)\n",
    "    shortcut = layers.BatchNormalization()(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "# Define ResNet model\n",
    "def resnet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv1D(16, kernel_size=3, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "    x = resnet_block(x, 32, 3, 2)\n",
    "    x = resnet_block(x, 32, 3, 1)\n",
    "    \n",
    "    x = resnet_block(x, 64, 3, 2)\n",
    "    x = resnet_block(x, 64, 3, 1)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = resnet(input_shape=(13, 1), num_classes=num_classes)\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.13.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\dell\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install utils \n",
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 7s 8ms/step - loss: 0.6562 - accuracy: 0.6237\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6219 - accuracy: 0.6471\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6129 - accuracy: 0.6560\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6135 - accuracy: 0.6514\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6112 - accuracy: 0.6623\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6089 - accuracy: 0.6544\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6111 - accuracy: 0.6602\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6090 - accuracy: 0.6654\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6060 - accuracy: 0.6736\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6077 - accuracy: 0.6671\n",
      "38/38 [==============================] - 1s 3ms/step\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 7s 8ms/step - loss: 0.6351 - accuracy: 0.6469\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6224 - accuracy: 0.6537\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 2s 15ms/step - loss: 0.6119 - accuracy: 0.6573\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.6112 - accuracy: 0.6648\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.6119 - accuracy: 0.6644\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6105 - accuracy: 0.6619\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6087 - accuracy: 0.6625\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 2s 12ms/step - loss: 0.6085 - accuracy: 0.6696\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6079 - accuracy: 0.6665\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.6026 - accuracy: 0.6704\n",
      "38/38 [==============================] - 1s 6ms/step\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 9s 9ms/step - loss: 0.6539 - accuracy: 0.6373\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 2s 12ms/step - loss: 0.6250 - accuracy: 0.6529\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.6209 - accuracy: 0.6512\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6148 - accuracy: 0.6587\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6123 - accuracy: 0.6564\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.6143 - accuracy: 0.6627\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6115 - accuracy: 0.6673\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.6108 - accuracy: 0.6635\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6075 - accuracy: 0.6733\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6054 - accuracy: 0.6708\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 7s 9ms/step - loss: 0.6392 - accuracy: 0.6421\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.6153 - accuracy: 0.6532\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6083 - accuracy: 0.6584\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6113 - accuracy: 0.6621\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6093 - accuracy: 0.6582\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6073 - accuracy: 0.6707\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6062 - accuracy: 0.6630\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6028 - accuracy: 0.6649\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6055 - accuracy: 0.6653\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6021 - accuracy: 0.6740\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 7s 10ms/step - loss: 0.6434 - accuracy: 0.6442\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6177 - accuracy: 0.6609\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6121 - accuracy: 0.6634\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6147 - accuracy: 0.6605\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6080 - accuracy: 0.6734\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 2s 12ms/step - loss: 0.6090 - accuracy: 0.6649\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.6071 - accuracy: 0.6665\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6046 - accuracy: 0.6642\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.6052 - accuracy: 0.6690\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.6050 - accuracy: 0.6736\n",
      "38/38 [==============================] - 1s 5ms/step\n",
      "Average accuracy: 62.12253951191936%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('audio_features10.csv')\n",
    "\n",
    "# Remove the empty columns\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Separate features and labels\n",
    "features = df[df.columns.difference(['label'])].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Convert labels to categorical\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# Reshape features for Conv1D Layer\n",
    "features = np.reshape(features, (features.shape[0], features.shape[1], 1))\n",
    "\n",
    "# Define ResNet block\n",
    "def resnet_block(inputs, num_filters, kernel_size, strides, activation='relu'):\n",
    "    x = layers.Conv1D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv1D(num_filters, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    shortcut = layers.Conv1D(num_filters, kernel_size=1, strides=strides, padding='same')(inputs)\n",
    "    shortcut = layers.BatchNormalization()(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "# Define ResNet model\n",
    "def create_model(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv1D(16, kernel_size=3, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "    x = resnet_block(x, 32, 3, 2)\n",
    "    x = resnet_block(x, 32, 3, 1)\n",
    "    \n",
    "    x = resnet_block(x, 64, 3, 2)\n",
    "    x = resnet_block(x, 64, 3, 1)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the StratifiedKFold object\n",
    "strat_k_fold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Create and train the LSTM model within each fold\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in strat_k_fold.split(features, np.argmax(labels, axis=1)):\n",
    "    features_train, features_test = features[train_index], features[test_index]\n",
    "    labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    model = create_model(input_shape=(features.shape[1], 1), num_classes=labels.shape[1])\n",
    "    model.fit(features_train, labels_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "    # Predict on the testing data\n",
    "    labels_pred = np.argmax(model.predict(features_test), axis=1)\n",
    "\n",
    "    # Measure accuracy of the model\n",
    "    accuracy = accuracy_score(np.argmax(labels_test, axis=1), labels_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Print the average accuracy\n",
    "print(f'Average accuracy: {np.mean(accuracies) * 100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 7s 9ms/step - loss: 0.6368 - accuracy: 0.6815\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5711 - accuracy: 0.7084\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5411 - accuracy: 0.7297\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5433 - accuracy: 0.7293\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4995 - accuracy: 0.7557\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.4780 - accuracy: 0.7715\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.4568 - accuracy: 0.7885\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.4267 - accuracy: 0.8023\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4215 - accuracy: 0.8125\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.3998 - accuracy: 0.8122\n",
      "20/20 [==============================] - 0s 4ms/step\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 6s 8ms/step - loss: 0.6283 - accuracy: 0.6531\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5939 - accuracy: 0.6807\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5796 - accuracy: 0.6823\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5683 - accuracy: 0.6989\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5444 - accuracy: 0.7297\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5305 - accuracy: 0.7407\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5230 - accuracy: 0.7336\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5094 - accuracy: 0.7482\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4915 - accuracy: 0.7597\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.4907 - accuracy: 0.7620\n",
      "20/20 [==============================] - 0s 4ms/step\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 6s 8ms/step - loss: 0.6360 - accuracy: 0.6472\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6069 - accuracy: 0.6744\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5974 - accuracy: 0.6831\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5872 - accuracy: 0.6902\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5651 - accuracy: 0.7044\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5679 - accuracy: 0.7028\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5634 - accuracy: 0.6997\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5484 - accuracy: 0.7115\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5455 - accuracy: 0.7167\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5060 - accuracy: 0.7419\n",
      "20/20 [==============================] - 0s 4ms/step\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 7s 10ms/step - loss: 0.6534 - accuracy: 0.6418\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.6071 - accuracy: 0.6679\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5887 - accuracy: 0.6789\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5688 - accuracy: 0.7002\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5614 - accuracy: 0.7010\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5621 - accuracy: 0.7085\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5632 - accuracy: 0.7073\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5472 - accuracy: 0.7219\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5368 - accuracy: 0.7298\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.5384 - accuracy: 0.7172\n",
      "20/20 [==============================] - 1s 5ms/step\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 7s 9ms/step - loss: 0.6357 - accuracy: 0.6785\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.5863 - accuracy: 0.6824\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.5765 - accuracy: 0.6899\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5682 - accuracy: 0.7022\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.5330 - accuracy: 0.7337\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4986 - accuracy: 0.7467\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.4888 - accuracy: 0.7613\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.4368 - accuracy: 0.7874\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3942 - accuracy: 0.8178\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.3304 - accuracy: 0.8536\n",
      "20/20 [==============================] - 0s 4ms/step\n",
      "Average accuracy: 44.76076566946243%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Read the data from the CSV file\n",
    "df = pd.read_csv('updated_voice.csv')\n",
    "\n",
    "# Remove the empty columns\n",
    "df = df.dropna(axis=1, how='all')\n",
    "\n",
    "# Separate features and labels\n",
    "features = df[df.columns.difference(['label'])].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Convert labels to categorical\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# Reshape features for Conv1D Layer\n",
    "features = np.reshape(features, (features.shape[0], features.shape[1], 1))\n",
    "\n",
    "# Define ResNet block\n",
    "def resnet_block(inputs, num_filters, kernel_size, strides, activation='relu'):\n",
    "    x = layers.Conv1D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv1D(num_filters, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    shortcut = layers.Conv1D(num_filters, kernel_size=1, strides=strides, padding='same')(inputs)\n",
    "    shortcut = layers.BatchNormalization()(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "# Define ResNet model\n",
    "def create_model(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv1D(16, kernel_size=3, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "    x = resnet_block(x, 32, 3, 2)\n",
    "    x = resnet_block(x, 32, 3, 1)\n",
    "    \n",
    "    x = resnet_block(x, 64, 3, 2)\n",
    "    x = resnet_block(x, 64, 3, 1)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the StratifiedKFold object\n",
    "strat_k_fold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Create and train the LSTM model within each fold\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in strat_k_fold.split(features, np.argmax(labels, axis=1)):\n",
    "    features_train, features_test = features[train_index], features[test_index]\n",
    "    labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    model = create_model(input_shape=(features.shape[1], 1), num_classes=labels.shape[1])\n",
    "    model.fit(features_train, labels_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "    # Predict on the testing data\n",
    "    labels_pred = np.argmax(model.predict(features_test), axis=1)\n",
    "\n",
    "    # Measure accuracy of the model\n",
    "    accuracy = accuracy_score(np.argmax(labels_test, axis=1), labels_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Print the average accuracy\n",
    "print(f'Average accuracy: {np.mean(accuracies) * 100}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first item in data: (13, 1)\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 12s 14ms/step - loss: 0.5409 - accuracy: 0.7384 - val_loss: 0.6679 - val_accuracy: 0.5713\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.4727 - accuracy: 0.7872 - val_loss: 0.6029 - val_accuracy: 0.7014\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.4519 - accuracy: 0.7947 - val_loss: 0.4871 - val_accuracy: 0.7473\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.4390 - accuracy: 0.8025 - val_loss: 0.5239 - val_accuracy: 0.7573\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.4341 - accuracy: 0.8083 - val_loss: 0.4650 - val_accuracy: 0.7890\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 2s 10ms/step - loss: 0.4200 - accuracy: 0.8116 - val_loss: 0.4606 - val_accuracy: 0.7940\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.4126 - accuracy: 0.8204 - val_loss: 0.6653 - val_accuracy: 0.6814\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 1s 10ms/step - loss: 0.4040 - accuracy: 0.8223 - val_loss: 0.4581 - val_accuracy: 0.7898\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.3977 - accuracy: 0.8277 - val_loss: 0.4928 - val_accuracy: 0.7832\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.3909 - accuracy: 0.8310 - val_loss: 0.4463 - val_accuracy: 0.7948\n",
      "Test loss: 0.44629451632499695\n",
      "Test accuracy: 0.7948290109634399\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "directory = r'F:\\data_ngcc'  # Replace with the directory path containing the .pt files\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(5993):\n",
    "    filename = f'{i}_ngcc.pt'\n",
    "    if filename in os.listdir(directory):  # Ensure the file exists in the directory\n",
    "        feature = torch.load(os.path.join(directory, filename))\n",
    "        data.append(feature.view(13,1).numpy())  # Adjusted this line to match ResNet input\n",
    "\n",
    "        labels.append(1 if i <= 2310 else 0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "data  = np.array(data)\n",
    "\n",
    "print(\"Shape of the first item in data:\", data[0].shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "# Define ResNet block\n",
    "def resnet_block(inputs, num_filters, kernel_size, strides, activation='relu'):\n",
    "    x = layers.Conv1D(num_filters, kernel_size=kernel_size, strides=strides, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv1D(num_filters, kernel_size=kernel_size, strides=1, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    shortcut = layers.Conv1D(num_filters, kernel_size=1, strides=strides, padding='same')(inputs)\n",
    "    shortcut = layers.BatchNormalization()(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    if activation:\n",
    "        x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "# Define ResNet model\n",
    "def resnet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Conv1D(16, kernel_size=3, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "    x = resnet_block(x, 32, 3, 2)\n",
    "    x = resnet_block(x, 32, 3, 1)\n",
    "    \n",
    "    x = resnet_block(x, 64, 3, 2)\n",
    "    x = resnet_block(x, 64, 3, 1)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = resnet(input_shape=(13, 1), num_classes=num_classes)\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first reshaped item in data: (1, 13)\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.7615 - accuracy: 0.7082 - val_loss: 0.6248 - val_accuracy: 0.7198\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.5193 - accuracy: 0.7787 - val_loss: 0.5957 - val_accuracy: 0.7331\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4937 - accuracy: 0.7908 - val_loss: 0.5880 - val_accuracy: 0.7515\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.4713 - accuracy: 0.8066 - val_loss: 0.5419 - val_accuracy: 0.7756\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4734 - accuracy: 0.7927 - val_loss: 0.5329 - val_accuracy: 0.7731\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4511 - accuracy: 0.8106 - val_loss: 0.5213 - val_accuracy: 0.7723\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4518 - accuracy: 0.8077 - val_loss: 0.5131 - val_accuracy: 0.7840\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4354 - accuracy: 0.8118 - val_loss: 0.5026 - val_accuracy: 0.7915\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 1s 5ms/step - loss: 0.4386 - accuracy: 0.8164 - val_loss: 0.5502 - val_accuracy: 0.7631\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 0.4292 - accuracy: 0.8225 - val_loss: 0.4948 - val_accuracy: 0.7915\n",
      "Test loss: 0.494801789522171\n",
      "Test accuracy: 0.7914929389953613\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "directory = r'F:\\data_gfcc'  # Replace with the directory path containing the .pt files\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(5993):\n",
    "    filename = f'{i}_gfcc.pt'\n",
    "    if filename in os.listdir(directory):  # Ensure the file exists in the directory\n",
    "        feature = torch.load(os.path.join(directory, filename))\n",
    "        # Reshape the feature to a 2D array with a single row and 40 columns\n",
    "        data.append(feature.view(1, -1).numpy())\n",
    "        # Assign labels: first 2312 files are labelled 1, rest are labelled 0\n",
    "        labels.append(1 if i <= 2310 else 0)\n",
    "\n",
    "\n",
    "labels = np.array(labels)\n",
    "data  = np.array(data)\n",
    "\n",
    "# Now you should be able to get a 3D shape from each element in data\n",
    "print(\"Shape of the first reshaped item in data:\", data[0].shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input data to match the CNN input shape\n",
    "input_shape = (data[0].shape[1], 1)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[2], 1)\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(labels))\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Now you can define and train your CNN as before\n",
    "# ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the CNN model\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first reshaped item in data: (13, 1)\n",
      "Shape of labels:  (5993,)\n",
      "Epoch 1/10\n",
      "150/150 [==============================] - 10s 26ms/step - loss: 0.5769 - accuracy: 0.6944 - val_loss: 0.5430 - val_accuracy: 0.7314\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 3s 20ms/step - loss: 0.5269 - accuracy: 0.7430 - val_loss: 0.5144 - val_accuracy: 0.7531\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 3s 19ms/step - loss: 0.4947 - accuracy: 0.7668 - val_loss: 0.4912 - val_accuracy: 0.7731\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 3s 21ms/step - loss: 0.4595 - accuracy: 0.7910 - val_loss: 0.4840 - val_accuracy: 0.7706\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 3s 20ms/step - loss: 0.4453 - accuracy: 0.8004 - val_loss: 0.4551 - val_accuracy: 0.7848\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 3s 21ms/step - loss: 0.4300 - accuracy: 0.8127 - val_loss: 0.4479 - val_accuracy: 0.7915\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 3s 19ms/step - loss: 0.4186 - accuracy: 0.8169 - val_loss: 0.4440 - val_accuracy: 0.7907\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 3s 17ms/step - loss: 0.4134 - accuracy: 0.8179 - val_loss: 0.4428 - val_accuracy: 0.7957\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 3s 17ms/step - loss: 0.4028 - accuracy: 0.8217 - val_loss: 0.4521 - val_accuracy: 0.7932\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 3s 17ms/step - loss: 0.4006 - accuracy: 0.8219 - val_loss: 0.4372 - val_accuracy: 0.7990\n",
      "Test loss: 0.43721824884414673\n",
      "Test accuracy: 0.7989991903305054\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dropout, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "directory = r'F:\\data_gfcc'  # Replace with the directory path containing the .pt files\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(5993):\n",
    "    filename = f'{i}_gfcc.pt'\n",
    "    if filename in os.listdir(directory):  # Ensure the file exists in the directory\n",
    "        feature = torch.load(os.path.join(directory, filename))\n",
    "        data.append(feature.view(-1, 1).numpy())  # Adjusted this line\n",
    "        labels.append(1 if i <= 2310 else 0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "data  = np.array(data)\n",
    "\n",
    "print(\"Shape of the first reshaped item in data:\", data[0].shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of labels: \", labels.shape)  # It should be (n_samples, n_classes)\n",
    "\n",
    "def get_model(op_nodes):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=32, return_sequences=True), input_shape=(data[0].shape[0], data[0].shape[1])))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Bidirectional(LSTM(units=32)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(op_nodes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the BiLSTM model\n",
    "model = get_model(num_classes)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "# Initialize whisper model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "directory = r'F:\\dataset_sri'\n",
    "output_directory= r'F:\\data_whisper'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_whisper_features(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    \n",
    "    # Load wav file\n",
    "    sample, sr = librosa.load(\"temp.wav\", sr = 16000)\n",
    "\n",
    "    # Process the wav file for the model\n",
    "    input_features = processor(sample, sampling_rate=sr, return_tensors=\"pt\").input_features\n",
    "    \n",
    "    # Extract the whisper features from the model\n",
    "    whisper_features = model(input_features).last_hidden_state[0].detach().numpy()\n",
    "    \n",
    "    return whisper_features\n",
    "\n",
    "for file in wav_files:\n",
    "    whisper_features = extract_whisper_features(file)\n",
    "    \n",
    "    # Convert array to torch tensor\n",
    "    whisper_features = torch.tensor(whisper_features)\n",
    "\n",
    "    # Construct output file name by replacing .m4a with .pt and changing the directory\n",
    "    output_file_whisper = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '_whisper.pt'))\n",
    "    \n",
    "    # Save to a PyTorch file (.pt)\n",
    "    torch.save(whisper_features, output_file_whisper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "from transformers.models.whisper.modeling_whisper import WhisperModel, WhisperEncoder\n",
    "from transformers.models.whisper.configuration_whisper import WhisperConfig\n",
    "from typing import Optional, Tuple, Union\n",
    "import torch\n",
    "import librosa \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, glob, pickle\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9add0650fda4e958bf25d4522077402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dell\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a556cb7a263c40b882788f7fe879acc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bae0ded20a94ecb8ce060e1ae63150e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93039dbab3eb454898f36afb3a5389b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940daa746a1f49a8bbe80d054f25e85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5484bee3af0b45ce9d4408557f9e9694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)main/normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031740464ec8445b85bfa234d08e01fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e441b1129e4887b0fb9bde2231fcff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f85a167cdd4e56bcfb772c63d70c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3c3f4f8ff641078f668431df8a8b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import WhisperConfig,  WhisperModel, WhisperProcessor\n",
    "\n",
    "class WhisperWordClassifier(WhisperModel):\n",
    "    def __init__(self, config: WhisperConfig):\n",
    "        super().__init__(config)\n",
    "        self.encoder = WhisperEncoder(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_features: Optional[torch.LongTensor] = None,  \n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_features,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        return encoder_outputs\n",
    "\n",
    "# Initialize whisper model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperWordClassifier.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "directory = r'F:\\dataset_sri'\n",
    "output_directory= r'F:\\data_whisper'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_whisper_features(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    \n",
    "    # Load wav file\n",
    "    sample, sr = librosa.load(\"temp.wav\", sr = 16000)\n",
    "\n",
    "    # Process the wav file for the model\n",
    "    input_features = processor(sample, sampling_rate=sr, return_tensors=\"pt\").input_features\n",
    "    \n",
    "    # Extract the whisper features from the model\n",
    "    whisper_features = model(input_features).last_hidden_state[0].detach().numpy()\n",
    "    \n",
    "    return whisper_features\n",
    "\n",
    "for file in wav_files:\n",
    "    whisper_features = extract_whisper_features(file)\n",
    "    \n",
    "    # Convert array to torch tensor\n",
    "    whisper_features = torch.tensor(whisper_features)\n",
    "\n",
    "    # Construct output file name by replacing .m4a with .pt and changing the directory\n",
    "    output_file_whisper = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '_whisper.pt'))\n",
    "    \n",
    "    # Save to a PyTorch file (.pt)\n",
    "    torch.save(whisper_features, output_file_whisper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 12.9 GiB for an array with shape (5993, 576000, 1) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m         labels\u001b[39m.\u001b[39mappend(\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2310\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m     20\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(labels)\n\u001b[1;32m---> 21\u001b[0m data  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(data)\n\u001b[0;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mShape of the first reshaped item in data:\u001b[39m\u001b[39m\"\u001b[39m, data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     25\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(data, labels, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 12.9 GiB for an array with shape (5993, 576000, 1) and data type float32"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Bidirectional, LSTM, Dropout, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "directory = r'F:\\data_whisper'  # Replace with the directory path containing the .pt files\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(5993):\n",
    "    filename = f'{i}_whisper.pt'\n",
    "    if filename in os.listdir(directory):  # Ensure the file exists in the directory\n",
    "        feature = torch.load(os.path.join(directory, filename))\n",
    "        data.append(feature.view(-1, 1).numpy())  # Adjusted this line\n",
    "        labels.append(1 if i <= 2310 else 0)\n",
    "\n",
    "labels = np.array(labels)\n",
    "data  = np.array(data)\n",
    "\n",
    "print(\"Shape of the first reshaped item in data:\", data[0].shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of labels: \", labels.shape)  # It should be (n_samples, n_classes)\n",
    "\n",
    "def get_model(op_nodes):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=32, return_sequences=True), input_shape=(data[0].shape[0], data[0].shape[1])))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Bidirectional(LSTM(units=32)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(op_nodes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the BiLSTM model\n",
    "model = get_model(num_classes)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(576000,1), n_classes=2, shuffle=True):\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            feature = torch.load(os.path.join(directory, f'{ID}_whisper.pt'))\n",
    "            X[i,] = feature.view(-1, 1).numpy()\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "# Define the model\n",
    "def get_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=32, return_sequences=True), input_shape=input_shape))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Bidirectional(LSTM(units=32)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Input shape based on the data\n",
    "input_shape = (576000, 1)\n",
    "num_classes = 2\n",
    "\n",
    "# Create the model\n",
    "model = get_model(input_shape, num_classes)\n",
    "\n",
    "# Parameters\n",
    "params = {'dim': (576000,1),\n",
    "          'batch_size': 32,\n",
    "          'n_classes': 2,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(range(2311, 5993), labels, **params)\n",
    "validation_generator = DataGenerator(range(2311), labels, **params)\n",
    "\n",
    "# Train model on dataset\n",
    "model.fit(training_generator, validation_data=validation_generator, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=60 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m feature \u001b[39m=\u001b[39m feature\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[39m# Apply PCA\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m feature_reduced \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(feature)\n\u001b[0;32m     24\u001b[0m \u001b[39m# Convert back to tensor and append to the list\u001b[39;00m\n\u001b[0;32m     25\u001b[0m data_reduced\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mfrom_numpy(feature_reduced))\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:462\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mC-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 462\u001b[0m U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m    463\u001b[0m U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:512\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_full(X, n_components)\n\u001b[0;32m    513\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_truncated(X, n_components, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:526\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components)\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    523\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmle\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is only supported if n_samples >= n_features\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    524\u001b[0m         )\n\u001b[0;32m    525\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n_components \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    527\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m must be between 0 and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmin(n_samples, n_features)=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    529\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msvd_solver=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n_components, \u001b[39mmin\u001b[39m(n_samples, n_features))\n\u001b[0;32m    530\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[39m# Center data\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=60 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "directory = r'F:\\data_whisper'  # Replace with your directory path\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=60)\n",
    "\n",
    "data_reduced = []\n",
    "\n",
    "for i in range(5993):\n",
    "    filename = f'{i}_whisper.pt'\n",
    "    if filename in os.listdir(directory):  # Ensure the file exists in the directory\n",
    "        # Load tensor and convert to numpy array\n",
    "        feature = torch.load(os.path.join(directory, filename)).numpy()\n",
    "        \n",
    "        # Reshape the feature to 2D (PCA requires 2D input)\n",
    "        feature = feature.reshape(1, -1)\n",
    "        \n",
    "        # Apply PCA\n",
    "        feature_reduced = pca.fit_transform(feature)\n",
    "        \n",
    "        # Convert back to tensor and append to the list\n",
    "        data_reduced.append(torch.from_numpy(feature_reduced))\n",
    "\n",
    "# Convert list to tensor\n",
    "data_reduced = torch.stack(data_reduced)\n",
    "\n",
    "# Now data_reduced contains the reduced data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5993\u001b[39m):\n\u001b[0;32m     15\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_whisper.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 16\u001b[0m     \u001b[39mif\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(directory):  \u001b[39m# Ensure the file exists in the directory\u001b[39;00m\n\u001b[0;32m     17\u001b[0m         \u001b[39m# Load tensor and convert to numpy array\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         feature \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, filename))\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     20\u001b[0m         \u001b[39m# Reshape the feature to 2D (PCA requires 2D input)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "directory = r'F:\\data_whisper'  # Replace with your directory path\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=60)\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "for i in range(5993):\n",
    "    filename = f'{i}_whisper.pt'\n",
    "    if filename in os.listdir(directory):  # Ensure the file exists in the directory\n",
    "        # Load tensor and convert to numpy array\n",
    "        feature = torch.load(os.path.join(directory, filename)).numpy()\n",
    "        \n",
    "        # Reshape the feature to 2D (PCA requires 2D input)\n",
    "        feature = feature.reshape(1, -1)\n",
    "        \n",
    "        # Append the data to the list\n",
    "        data.append(feature)\n",
    "\n",
    "# Convert list to a numpy array\n",
    "data = np.concatenate(data, axis=0)\n",
    "\n",
    "# Apply PCA to all data\n",
    "data_reduced = pca.fit_transform(data)\n",
    "\n",
    "# Convert numpy array back to tensor\n",
    "data_reduced = torch.from_numpy(data_reduced)\n",
    "\n",
    "# Now data_reduced contains the reduced data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=60 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m whisper_features\n\u001b[0;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m wav_files:\n\u001b[1;32m---> 69\u001b[0m     whisper_features \u001b[39m=\u001b[39m extract_whisper_features(file)\n\u001b[0;32m     71\u001b[0m     \u001b[39m# Convert array to torch tensor\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     whisper_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(whisper_features)\n",
      "Cell \u001b[1;32mIn[6], line 64\u001b[0m, in \u001b[0;36mextract_whisper_features\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m     61\u001b[0m whisper_features \u001b[39m=\u001b[39m model(input_features)\u001b[39m.\u001b[39mlast_hidden_state[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     63\u001b[0m \u001b[39m# Reduce dimensionality with PCA\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m whisper_features \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(whisper_features\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     66\u001b[0m \u001b[39mreturn\u001b[39;00m whisper_features\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:462\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mC-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 462\u001b[0m U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m    463\u001b[0m U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:512\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_full(X, n_components)\n\u001b[0;32m    513\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_truncated(X, n_components, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:526\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components)\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    523\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmle\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is only supported if n_samples >= n_features\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    524\u001b[0m         )\n\u001b[0;32m    525\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n_components \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    527\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m must be between 0 and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmin(n_samples, n_features)=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    529\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msvd_solver=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n_components, \u001b[39mmin\u001b[39m(n_samples, n_features))\n\u001b[0;32m    530\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[39m# Center data\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=60 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import librosa\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import WhisperConfig, WhisperModel, WhisperProcessor\n",
    "\n",
    "class WhisperWordClassifier(WhisperModel):\n",
    "    def __init__(self, config: WhisperConfig):\n",
    "        super().__init__(config)\n",
    "        self.encoder = WhisperEncoder(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_features: Optional[torch.LongTensor] = None,  \n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_features,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        return encoder_outputs\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=60)  # You can change this to another value as desired\n",
    "\n",
    "# Initialize whisper model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperWordClassifier.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "directory = r'F:\\dataset_sri'\n",
    "output_directory= r'F:\\data_whisper2'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_whisper_features(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    \n",
    "    # Load wav file\n",
    "    sample, sr = librosa.load(\"temp.wav\", sr = 16000)\n",
    "\n",
    "    # Process the wav file for the model\n",
    "    input_features = processor(sample, sampling_rate=sr, return_tensors=\"pt\").input_features\n",
    "    \n",
    "    # Extract the whisper features from the model\n",
    "    whisper_features = model(input_features).last_hidden_state[0].detach().numpy()\n",
    "    \n",
    "    # Reduce dimensionality with PCA\n",
    "    whisper_features = pca.fit_transform(whisper_features.reshape(1, -1))\n",
    "    \n",
    "    return whisper_features\n",
    "\n",
    "for file in wav_files:\n",
    "    whisper_features = extract_whisper_features(file)\n",
    "    \n",
    "    # Convert array to torch tensor\n",
    "    whisper_features = torch.tensor(whisper_features)\n",
    "\n",
    "    # Construct output file name by replacing .m4a with .pt and changing the directory\n",
    "    output_file_whisper = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '_whisper.pt'))\n",
    "    \n",
    "    # Save to a PyTorch file (.pt)\n",
    "    torch.save(whisper_features, output_file_whisper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 5ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 905us/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 1ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[39mreturn\u001b[39;00m whisper_features_reduced\n\u001b[0;32m     85\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m wav_files:\n\u001b[1;32m---> 86\u001b[0m     whisper_features \u001b[39m=\u001b[39m extract_whisper_features(file)\n\u001b[0;32m     88\u001b[0m     \u001b[39m# Convert array to torch tensor\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     whisper_features \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(whisper_features)\n",
      "Cell \u001b[1;32mIn[8], line 78\u001b[0m, in \u001b[0;36mextract_whisper_features\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m     75\u001b[0m whisper_features \u001b[39m=\u001b[39m (whisper_features \u001b[39m-\u001b[39m whisper_features\u001b[39m.\u001b[39mmin()) \u001b[39m/\u001b[39m (whisper_features\u001b[39m.\u001b[39mmax() \u001b[39m-\u001b[39m whisper_features\u001b[39m.\u001b[39mmin())\n\u001b[0;32m     77\u001b[0m \u001b[39m# Train the autoencoder\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m autoencoder\u001b[39m.\u001b[39;49mfit(whisper_features, whisper_features, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     80\u001b[0m \u001b[39m# Use the encoder to reduce dimensionality\u001b[39;00m\n\u001b[0;32m     81\u001b[0m whisper_features_reduced \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mpredict(whisper_features)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import WhisperConfig,  WhisperModel, WhisperProcessor\n",
    "\n",
    "class WhisperWordClassifier(WhisperModel):\n",
    "    def __init__(self, config: WhisperConfig):\n",
    "        super().__init__(config)\n",
    "        self.encoder = WhisperEncoder(config)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_features: Optional[torch.LongTensor] = None,  \n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        encoder_outputs = self.encoder(\n",
    "            input_features,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        return encoder_outputs\n",
    "\n",
    "# Initialize autoencoder\n",
    "# Initialize autoencoder\n",
    "input_size = 384  # change this to the actual output size of the Whisper model\n",
    "encoding_dim = 60\n",
    "input_layer = Input(shape=(input_size,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(input_size, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "encoder = Model(input_layer, encoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "# Initialize whisper model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n",
    "model = WhisperWordClassifier.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "\n",
    "directory = r'F:\\dataset_sri'\n",
    "output_directory= r'F:\\data_whisper2'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def extract_whisper_features(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    \n",
    "    # Load wav file\n",
    "    sample, sr = librosa.load(\"temp.wav\", sr = 16000)\n",
    "\n",
    "    # Process the wav file for the model\n",
    "    input_features = processor(sample, sampling_rate=sr, return_tensors=\"pt\").input_features\n",
    "    \n",
    "    # Extract the whisper features from the model\n",
    "    whisper_features = model(input_features).last_hidden_state[0].detach().numpy()\n",
    "    \n",
    "    # Normalize the features to [0, 1] range as the autoencoder uses sigmoid activation\n",
    "    whisper_features = (whisper_features - whisper_features.min()) / (whisper_features.max() - whisper_features.min())\n",
    "\n",
    "    # Train the autoencoder\n",
    "    autoencoder.fit(whisper_features, whisper_features, epochs=50, batch_size=256, shuffle=True, verbose=0)\n",
    "\n",
    "    # Use the encoder to reduce dimensionality\n",
    "    whisper_features_reduced = encoder.predict(whisper_features)\n",
    "    \n",
    "    return whisper_features_reduced\n",
    "\n",
    "for file in wav_files:\n",
    "    whisper_features = extract_whisper_features(file)\n",
    "    \n",
    "    # Convert array to torch tensor\n",
    "    whisper_features = torch.tensor(whisper_features)\n",
    "\n",
    "    # Construct output file name by replacing .m4a with .pt and changing the directory\n",
    "    output_file_whisper = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '_whisper.pt'))\n",
    "    \n",
    "    # Save to a PyTorch file (.pt)\n",
    "    torch.save(whisper_features, output_file_whisper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_16076\\4292180509.py:26: RuntimeWarning: invalid value encountered in divide\n",
      "  group_delay = np.diff(phase, axis=-1) / np.diff(magnitude, axis=-1)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_16076\\4292180509.py:26: RuntimeWarning: divide by zero encountered in divide\n",
      "  group_delay = np.diff(phase, axis=-1) / np.diff(magnitude, axis=-1)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_16076\\4292180509.py:30: RuntimeWarning: invalid value encountered in multiply\n",
      "  mgd = magnitude * group_delay\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.fftpack as fft\n",
    "import scipy.io.wavfile as wav\n",
    "import os\n",
    "import fnmatch\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "\n",
    "directory = r'F:\\dataset_sri'\n",
    "output_directory= r'F:\\data_mgdcc'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "wav_files = [os.path.join(root, file)\n",
    "             for root, dirs, files in os.walk(directory)\n",
    "             for file in fnmatch.filter(files, '*.m4a')]\n",
    "\n",
    "def calculate_mgdcc(data, samplerate, n_fft=512, hop_length=256, n_mels=128, n_mfcc=13):\n",
    "    # Compute the short-time Fourier transform\n",
    "    stft = librosa.stft(data, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    # Calculate the magnitude and phase spectra\n",
    "    magnitude = np.abs(stft)\n",
    "    phase = np.unwrap(np.angle(stft))\n",
    "\n",
    "    # Compute the group delay function\n",
    "    group_delay = np.diff(phase, axis=-1) / np.diff(magnitude, axis=-1)\n",
    "    group_delay = np.pad(group_delay, ((0, 0), (0, 1)), mode='constant')\n",
    "\n",
    "    # Compute the modified group delay function\n",
    "    mgd = magnitude * group_delay\n",
    "\n",
    "    # Take the DCT of the MGDF to get MGDCCs\n",
    "    mgdcc = fft.dct(mgd, axis=-1)\n",
    "\n",
    "    # Mean normalization\n",
    "    mgdcc_processed = np.mean(mgdcc, axis=0)\n",
    "\n",
    "    return mgdcc_processed\n",
    "\n",
    "def extract_mgdcc(file_name):\n",
    "    # Convert m4a file to wav            \n",
    "    audio = AudioSegment.from_file(file_name)\n",
    "    audio.export(\"temp.wav\", format=\"wav\")\n",
    "    (samplerate, data) = wav.read(\"temp.wav\")\n",
    "    \n",
    "    # Convert to floating point values ranging from -1 to 1\n",
    "    data = data / 32767.0\n",
    "    \n",
    "    mgdccs = calculate_mgdcc(data, samplerate)\n",
    "    \n",
    "    return  mgdccs\n",
    "\n",
    "for file in wav_files:\n",
    "    mgdccs = extract_mgdcc(file)\n",
    "    \n",
    "    # Convert array to torch tensor\n",
    "    mgdccs = torch.tensor(mgdccs)\n",
    "\n",
    "    # Construct output file name by replacing .wav with .pt and changing the directory\n",
    "    output_file_mgdcc = os.path.join(output_directory, os.path.basename(file).replace('.m4a', '_mgdcc.pt'))\n",
    "    \n",
    "    # Save to a PyTorch file (.pt)\n",
    "    torch.save(mgdccs, output_file_mgdcc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
